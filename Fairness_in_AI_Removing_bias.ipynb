{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness in AI: Removing word embeddings\n",
    "\n",
    "#### Kylian van Geijtenbeek, Thom Visser, Martine Toering, Iulia Ionescu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Abstract:}$ In this paper we reproduce the word embedding debiasing algorithm from Bolukbasi et al. [2]. We adapt the available implementation and extend it with their soft debiasing method. We integrate several popular benchmarks and investigate the effectiveness of the algorithm on GloVe and fastText embeddings besides the Word2vec embeddings used by Bolukbasi et al. [2]. We show that the removal of direct bias from all the different embeddings barely affects their effectiveness through a comparison of benchmark scores. However, we fail to reproduce the large scale soft debiasing results due to a lack of detail on the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import embetter as dwe\n",
    "import embetter.we as we\n",
    "from embetter.we import WordEmbedding\n",
    "from embetter.data import *\n",
    "\n",
    "from embetter.debias import *\n",
    "from embetter.benchmarks import Benchmark\n",
    "\n",
    "from compare_bias import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Gender Bias in word2vec, Glove and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use three different word embeddings: $\\textbf{word2vec}$ (Mikolov et al. 2013), $\\textbf{glove}$ (Pennington et al. 2014) and $\\textbf{fastText}$ (Bojanowski et al. 2016).\n",
    "\n",
    "The word2vec embedding we use is learned from a corpus of Google News articles (https://code.google.com/archive/p/word2vec/). The embeddings are 300-dimensional for 3 million words. For glove we make use of the 300-dimensional vectors trained on Common Crawl (https://nlp.stanford.edu/projects/glove/). Lastly, FastText is a word embedding from Facebook AI Research lab trained on Wikipedia corpus and Common Crawl and also consists of 300-dimensional vectors (https://fasttext.cc/docs/en/english-vectors.html).\n",
    "\n",
    "We start by loading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word2vec_small to /Users/iulia/Documents/M_AI/FACT/FactAI/embetter/embeddings/word2vec_small.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2540/2540 [00:01<00:00, 1773.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "Downloading word2vec_small_hard_debiased to /Users/iulia/Documents/M_AI/FACT/FactAI/embetter/embeddings/word2vec_small_hard_debiased.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2953/2953 [00:04<00:00, 716.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "Downloading word2vec_small_soft_debiased to /Users/iulia/Documents/M_AI/FACT/FactAI/embetter/embeddings/word2vec_small_soft_debiased.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2952/2952 [00:00<00:00, 6723.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n"
     ]
    }
   ],
   "source": [
    "# Load google news word2vec\n",
    "E = WordEmbedding(\"word2vec_small\")\n",
    "# Load debiased word2vec\n",
    "E_hard = WordEmbedding(\"word2vec_small_hard_debiased\")\n",
    "E_soft = WordEmbedding(\"word2vec_small_soft_debiased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove\n",
    "E_g = WordEmbedding(\"glove_small\")\n",
    "E_g_hard = WordEmbedding(\"glove_small_hard_debiased\")\n",
    "E_g_soft = WordEmbedding(\"glove_small_soft_debiased\")\n",
    "\n",
    "\n",
    "# Load FastText \n",
    "E_f = WordEmbedding(\"fasttext_small\")\n",
    "E_f_hard = WordEmbedding(\"fasttext_small_hard_debiased\")\n",
    "E_f_soft = WordEmbedding(\"fasttext_small_soft_debiased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load professions and gender related lists from Bolukbasi et al. for word2vec\n",
    "\n",
    "gender_seed, defs, equalize_pairs, profession_words = load_data(E.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define gender direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the gender direction by either PCA or by the words \"she\" and \"he\" for word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gender direction with the words \"she\" and \"he\" \n",
    "# v_gender = E.diff('she', 'he')\n",
    "\n",
    "# Define gender direction with PCA\n",
    "v_gender = we.doPCA(defs, E).components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating analogies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show some of the gender analogies that we can create from the embeddings. TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies gender\n",
    "a_gender = E.best_analogies_dist_thresh(v_gender, thresh=1)\n",
    "we.viz(a_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing occupational gender bias \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of extreme male and extreme female professions\n",
    "sp = E.profession_stereotypes(profession_words, v_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load professions and gender related lists from Bolukbasi et al. for fastText\n",
    "gender_seed, defs, equalize_pairs, profession_words = load_data(E_f.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define gender direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the gender direction by either PCA or by the words \"she\" and \"he\" for fastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gender direction with the words \"she\" and \"he\" \n",
    "# v_gender = E_f.diff('she', 'he')\n",
    "\n",
    "# Define gender direction with PCA\n",
    "v_gender = we.doPCA(defs, E_f).components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating analogies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies gender\n",
    "a_gender = E_f.best_analogies_dist_thresh(v_gender, thresh=1)\n",
    "we.viz(a_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing occupational gender bias \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis of extreme male and extreme female professions\n",
    "sp = E_f.profession_stereotypes(profession_words, v_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Comparing Bias of word2vec, Glove and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the gender bias between word embeddings FastText and Glove. We do this by following Bolukbasi et al. approach on figure 4 in their paper. The profession words are projected onto the gender axis for FastText and Glove. Each datapoint represents a profession word.\n",
    "\n",
    "Below we compare the bias of embeddings Word2vec and fastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_occupational_bias(E, E_f, [\"Word2vec\", \"FastText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_occupational_bias(E, E_f, [\"Glove\", \"FastText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3 - Debiasing algorithms on word2vec, Glove and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard debiasing\n",
    "\n",
    "In hard debiasing, the gender neutral words are shifted to zero in the gender subspace (i.e. neutralized) by subtracting the projection of the neutral word embedding vector onto the gender subspace and renormalizing the resulting embedding to unit length. \n",
    "\n",
    "## Soft debiasing\n",
    "\n",
    "We adapted specifics from Manzini et al., Soft debiasing is done by solving the following optimization problem as mentioned in their paper:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{T}{\\min} || (TW)^T(TW) - W^TW||^2_F + \\lambda ||(TN)^T (TB)||^2_F\n",
    "\\end{equation}\n",
    "\n",
    "where W is the matrix of all embedding vectors, N is the matrix of the embedding vectors of the gender neutral words, B is the gender subspace, and T is the debiasing transformation that minimizes the projection of the neutral words onto the gender subspace but tries to maintain the pairwise inner products between the words.\n",
    "\n",
    "This code is largely based on code from https://github.com/TManzini/DebiasMulticlassWordEmbedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard debiasing Word2vec\n",
    "First, we show the effect of hard debiasing on Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard debiased Word2vec\n",
    "# Analysis of extreme male and extreme female professions\n",
    "sp_hard_debiased = E_hard.profession_stereotypes(profession_words, v_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies gender\n",
    "a_gender_hard_debiased = E_hard.best_analogies_dist_thresh(v_gender)\n",
    "we.viz(a_gender_hard_debiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(benchmark, E, E_hard, E_soft, embedding_name):\n",
    "    result_original = benchmark.evaluate(E, \"'Before', {}\".format(embedding_name))\n",
    "    result_hard_debiased = benchmark.evaluate(E_hard, \"'Hard debiased', {}\".format(embedding_name))\n",
    "    result_soft_debiased = benchmark.evaluate(E_soft, \"'Soft debiased', {}\".format(embedding_name))\n",
    "    results = [result_original, result_hard_debiased, result_soft_debiased]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show the benchmarks for Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate for word2vec\n",
    "w2v_benchmark = Benchmark()\n",
    "w2v_results = run_benchmark(w2v_benchmark, E, E_hard, E_soft, \"word2vec\")\n",
    "w2v_benchmark.pprint_compare(w2v_results, [\"Before\", \"Hard-debiased\", \"Soft-debiased\"], \"word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove and FastText\n",
    "g_benchmark = Benchmark()\n",
    "g_results = run_benchmark(g_benchmark, E_g, E_g_hard, E_g_soft, \"Glove\")\n",
    "g_benchmark.pprint_compare(g_results, [\"Before\", \"Hard-debiased\", \"Soft-debiased\"], \"Glove\")\n",
    "\n",
    "f_benchmark = Benchmark()\n",
    "f_results = run_benchmark(f_benchmark, E_f, E_f_hard, E_f_soft, \"fastText\")\n",
    "f_benchmark.pprint_compare(f_results, [\"Before\", \"Hard-debiased\", \"Soft-debiased\"], \"fastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
