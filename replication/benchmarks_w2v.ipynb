{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Standard Benchmarks: Coherence\n",
    "### Using evaluation tool for word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply standard benchmarks on coherence on w2v and debiased w2v.\n",
    "\n",
    "Sources:\n",
    "\n",
    "#### RG: H. Rubenstein and J. B. Goodenough. Contextual correlates of synonymy. Communications of the ACM, 8(10):627â€“633, 1965.\n",
    "\n",
    "####  WS: L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search in context: The concept  revisited. In WWW. ACM, 2001.\n",
    "\n",
    "####  Wordsim benchmarks - Code adapted from source - embedding-evaluation: https://github.com/k-kawakami/embedding-evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of GoogleNews-vectors:\n",
    "# https://drive.google.com/file/d/1NH6jcrg8SXbnhpIXRIXF_-KUE7wGxGaG/view?usp=sharing\n",
    "\n",
    "# For full embeddings:\n",
    "# Download embeddings at https://github.com/tolga-b/debiaswe and put them on the following directory\n",
    "# embeddings/GoogleNews-vectors-negative300-hard-debiased.bin\n",
    "# embeddings/GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions\n",
    "\n",
    "from debiaswe.benchmarks import Wordsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small w2vNEWS set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: original word embeddings on RG & WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "false\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "+-------------------------------------------------------+\n",
      "|                  Results for 'Before'                 |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |    Score (rho)    |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  318  |     35    | 68.82719646959825 |\n",
      "| EN-RG-65      |   53  |     12    | 77.66555804950227 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Load google news word2vec\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "# Evaluate\n",
    "wordsim = Wordsim()\n",
    "word2vec = E.get_dict()\n",
    "result_original = wordsim.evaluate(word2vec, \"'Before'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2: Debiased word embeddings on RG & WS\n",
    "\n",
    "\n",
    "### Step 2a: Hard debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debiaswe.debias import debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "    gender_specific_words = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "debias(E, gender_specific_words, defs, equalize_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|              Results for 'Hard-debiased'              |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |    Score (rho)    |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  318  |     35    | 68.52623098234018 |\n",
      "| EN-RG-65      |   53  |     12    | 77.49622028082247 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for hard-debiased\n",
    "word2vec_debiased = E.get_dict()\n",
    "result_debiased = wordsim.evaluate(word2vec_debiased, \"'Hard-debiased'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|               Results for small dataset               |\n",
      "+---------------+-------------------+-------------------+\n",
      "|  Score (rho)  |      EN-RG-65     |   EN-WS-353-ALL   |\n",
      "+---------------+-------------------+-------------------+\n",
      "|     Before    | 68.82719646959825 | 77.66555804950227 |\n",
      "| Hard-debiased | 68.52623098234018 | 77.49622028082247 |\n",
      "+---------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "wordsim.pprint_w2vnews([result_original, result_debiased], [\"Before\", \"Hard-debiased\"], \"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full W2vNEWS set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: original word embeddings on RG & WS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordsim benchmarks\n",
    "Code adapted from source \n",
    "\n",
    "#### embedding-evaluation: https://github.com/k-kawakami/embedding-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/GoogleNews-vectors-negative300.bin\n",
      "true\n",
      "(3000000, 300)\n",
      "3000000 words of dimension 300 : </s>, in, for, that, ..., Bim_Skala_Bim, Mezze_Cafe, pulverizes_boulders, snowcapped_Caucasus\n",
      "3000000 words of dimension 300 : </s>, in, for, that, ..., Bim_Skala_Bim, Mezze_Cafe, pulverizes_boulders, snowcapped_Caucasus\n",
      "+-------------------------------------------------------+\n",
      "|                  Results for 'Before'                 |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |    Score (rho)    |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  353  |     0     | 69.99554084579108 |\n",
      "| EN-RG-65      |   65  |     0     | 76.11190062960836 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Load google news word2vec\n",
    "E = WordEmbedding('./embeddings/GoogleNews-vectors-negative300.bin')\n",
    "# Evaluate\n",
    "wordsim = Wordsim()\n",
    "word2vec = E.get_dict()\n",
    "result_original = wordsim.evaluate(word2vec, \"'Before'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2: Debiased word embeddings on RG & WS\n",
    "\n",
    "\n",
    "### Step 2a: Hard debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debiaswe.debias import debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f)\n",
    "\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "    gender_specific_words = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "debias(E, gender_specific_words, defs, equalize_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|              Results for 'Hard-debiased'              |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |    Score (rho)    |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  353  |     0     | 69.75245575417881 |\n",
      "| EN-RG-65      |   65  |     0     | 76.50009325498166 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for hard-debiased\n",
    "word2vec_debiased = E.get_dict()\n",
    "result_debiased = wordsim.evaluate(word2vec_debiased, \"'Hard-debiased'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|                Results for full dataset               |\n",
      "+---------------+-------------------+-------------------+\n",
      "|  Score (rho)  |      EN-RG-65     |   EN-WS-353-ALL   |\n",
      "+---------------+-------------------+-------------------+\n",
      "|     Before    | 69.99554084579108 | 76.11190062960836 |\n",
      "| Hard-debiased | 69.75245575417881 | 76.50009325498166 |\n",
      "+---------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "wordsim.pprint_w2vnews([result_original, result_debiased], [\"Before\", \"Hard-debiased\"], \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
