{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Standard Benchmarks: Coherence\n",
    "### Using evaluation tool for word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply standard benchmarks on coherence on w2v and debiased w2v.\n",
    "\n",
    "Sources:\n",
    "\n",
    "#### RG: H. Rubenstein and J. B. Goodenough. Contextual correlates of synonymy. Communications of the ACM, 8(10):627â€“633, 1965.\n",
    "\n",
    "####  WS: L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search in context: The concept  revisited. In WWW. ACM, 2001.\n",
    "\n",
    "####  Wordsim benchmarks - Code adapted from source - embedding-evaluation: https://github.com/k-kawakami/embedding-evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of GoogleNews-vectors:\n",
    "# https://drive.google.com/file/d/1NH6jcrg8SXbnhpIXRIXF_-KUE7wGxGaG/view?usp=sharing\n",
    "\n",
    "# For full embeddings:\n",
    "# Download embeddings at https://github.com/tolga-b/debiaswe and put them on the following directory\n",
    "# embeddings/GoogleNews-vectors-negative300-hard-debiased.bin\n",
    "# embeddings/GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions\n",
    "\n",
    "from debiaswe.benchmarks import Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: original word embeddings on RG & WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/fasttext_wiki-news-300d-1M.vec\n",
      "Got weird line 999994 300\n",
      "\n",
      "(50000, 300)\n",
      "50000 words of dimension 300 : ,, the, ., and, ..., Winston-Salem, anion, JSP, Postecoglou\n",
      "50000 words of dimension 300 : ,, the, ., and, ..., Winston-Salem, anion, JSP, Postecoglou\n",
      "29950 words of dimension 300 : ,, the, ., and, ..., circumscribed, whos, salvaging, anion\n",
      "longer than 19: 3 next: 29947\n",
      "29947 words of dimension 300 : ,, the, ., and, ..., circumscribed, whos, salvaging, anion\n",
      "count dig words: 1780 so next: 28167\n",
      "28167 words of dimension 300 : ,, the, ., and, ..., circumscribed, whos, salvaging, anion\n",
      "count punctuation words: 1153 so next: 27014\n",
      "27014 words of dimension 300 : the, and, of, to, ..., circumscribed, whos, salvaging, anion\n",
      "Wrote 27014 words to ./embeddings/fasttext_wiki-news-300d_small.txt\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# Obtain small FastText embeddings set\n",
    "E = WordEmbedding('./embeddings/fasttext_wiki-news-300d-1M.vec', limit=50000)\n",
    "E.filter_words(lambda x: x.lower()==x)\n",
    "\n",
    "count_long = 0\n",
    "for word in E.words:\n",
    "    if len(word)>19:\n",
    "        count_long += 1 \n",
    "print(\"longer than 19:\", count_long, \"next:\", len(E.words) - count_long)\n",
    "        \n",
    "E.filter_words(lambda x: len(x)<20)\n",
    "\n",
    "count_dig_words = 0\n",
    "for word in E.words:\n",
    "    dig = False\n",
    "    for c in word:\n",
    "        if c.isdigit():\n",
    "            dig = True\n",
    "    if dig:\n",
    "        count_dig_words += 1\n",
    "print(\"count dig words:\", count_dig_words, \"so next:\", len(E.words) - count_dig_words)\n",
    "        \n",
    "E.filter_words(lambda x: not any((char.isdigit() for char in x)))\n",
    "\n",
    "count = 0\n",
    "for word in E.words:\n",
    "    char = False\n",
    "    for c in word:\n",
    "        if c in string.punctuation:\n",
    "            char = True\n",
    "    if char:\n",
    "        count += 1\n",
    "print(\"count punctuation words:\", count, \"so next:\", len(E.words) - count)\n",
    "\n",
    "E.filter_words((lambda x: not any((char in set(string.punctuation)) for char in x)))\n",
    "E.save('./embeddings/fasttext_wiki-news-300d_small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 40\n",
      "Processing batch 2 of 40\n",
      "Processing batch 3 of 40\n",
      "Processing batch 4 of 40\n",
      "Processing batch 5 of 40\n",
      "Processing batch 6 of 40\n",
      "Processing batch 7 of 40\n",
      "Processing batch 8 of 40\n",
      "Processing batch 9 of 40\n",
      "Processing batch 10 of 40\n",
      "Processing batch 11 of 40\n",
      "Processing batch 12 of 40\n",
      "Processing batch 13 of 40\n",
      "Processing batch 14 of 40\n",
      "Processing batch 15 of 40\n",
      "Processing batch 16 of 40\n",
      "Processing batch 17 of 40\n",
      "Processing batch 18 of 40\n",
      "Processing batch 19 of 40\n",
      "Processing batch 20 of 40\n",
      "Processing batch 21 of 40\n",
      "Processing batch 22 of 40\n",
      "Processing batch 23 of 40\n",
      "Processing batch 24 of 40\n",
      "Processing batch 25 of 40\n",
      "Processing batch 26 of 40\n",
      "Processing batch 27 of 40\n",
      "Processing batch 28 of 40\n",
      "Processing batch 29 of 40\n",
      "Processing batch 30 of 40\n",
      "Processing batch 31 of 40\n",
      "Processing batch 32 of 40\n",
      "Processing batch 33 of 40\n",
      "Processing batch 34 of 40\n",
      "Processing batch 35 of 40\n",
      "Processing batch 36 of 40\n",
      "Processing batch 37 of 40\n",
      "Processing batch 38 of 40\n",
      "Processing batch 39 of 40\n",
      "Processing batch 40 of 40\n",
      "+-------------------------------------------------------+\n",
      "|             Results for 'Before', FastText            |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |       Score       |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  324  |     29    | 74.10786418997199 |\n",
      "| EN-RG-65      |   56  |     9     | 83.86622701863348 |\n",
      "| MSR-analogy   |  4838 |    3162   | 55.87019429516329 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "benchmark = Benchmark()\n",
    "result_original = benchmark.evaluate(E, \"'Before', FastText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2: Debiased word embeddings on RG & WS\n",
    "\n",
    "\n",
    "### Step 2a: Hard debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debiaswe.debias import hard_debias\n",
    "\n",
    "# Path for hard_debiased embedding file \n",
    "hard_embedding_file = './embeddings/fasttext_wiki-news-300d-1M_hard_debiased.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def pairs size: 10\n",
      "defs size: 9\n",
      "27014 words of dimension 300 : the, and, of, to, ..., circumscribed, whos, salvaging, anion\n",
      "{('FRATERNITY', 'SORORITY'), ('fella', 'granny'), ('Grandfather', 'Grandmother'), ('Gelding', 'Mare'), ('COLT', 'FILLY'), ('Schoolboy', 'Schoolgirl'), ('CHAIRMAN', 'CHAIRWOMAN'), ('grandfather', 'grandmother'), ('SPOKESMAN', 'SPOKESWOMAN'), ('male', 'female'), ('MALES', 'FEMALES'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('Grandson', 'Granddaughter'), ('KINGS', 'QUEENS'), ('brothers', 'sisters'), ('BOYS', 'GIRLS'), ('fraternity', 'sorority'), ('prostate_cancer', 'ovarian_cancer'), ('Father', 'Mother'), ('BOY', 'GIRL'), ('PRINCE', 'PRINCESS'), ('MAN', 'WOMAN'), ('testosterone', 'estrogen'), ('GRANDPA', 'GRANDMA'), ('Uncle', 'Aunt'), ('men', 'women'), ('Nephew', 'Niece'), ('Gentleman', 'Lady'), ('twin_brother', 'twin_sister'), ('Prince', 'Princess'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('Sons', 'Daughters'), ('councilman', 'councilwoman'), ('gentlemen', 'ladies'), ('congressman', 'congresswoman'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('catholic_priest', 'nun'), ('nephew', 'niece'), ('Dudes', 'Gals'), ('BROTHERS', 'SISTERS'), ('Boy', 'Girl'), ('kings', 'queens'), ('Monastery', 'Convent'), ('GELDING', 'MARE'), ('Himself', 'Herself'), ('MALE', 'FEMALE'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('Dads', 'Moms'), ('DAD', 'MOM'), ('wives', 'husbands'), ('Fraternity', 'Sorority'), ('FATHER', 'MOTHER'), ('grandsons', 'granddaughters'), ('Chairman', 'Chairwoman'), ('man', 'woman'), ('Brothers', 'Sisters'), ('HE', 'SHE'), ('Male', 'Female'), ('GENTLEMEN', 'LADIES'), ('Fella', 'Granny'), ('FELLA', 'GRANNY'), ('NEPHEW', 'NIECE'), ('Councilman', 'Councilwoman'), ('DUDES', 'GALS'), ('Males', 'Females'), ('SONS', 'DAUGHTERS'), ('SON', 'DAUGHTER'), ('himself', 'herself'), ('HIS', 'HER'), ('KING', 'QUEEN'), ('dad', 'mom'), ('Wives', 'Husbands'), ('Fatherhood', 'Motherhood'), ('chairman', 'chairwoman'), ('Grandpa', 'Grandma'), ('businessman', 'businesswoman'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('gelding', 'mare'), ('spokesman', 'spokeswoman'), ('Gentlemen', 'Ladies'), ('grandson', 'granddaughter'), ('schoolboy', 'schoolgirl'), ('Fathers', 'Mothers'), ('uncle', 'aunt'), ('monastery', 'convent'), ('Men', 'Women'), ('HIMSELF', 'HERSELF'), ('CATHOLIC_PRIEST', 'NUN'), ('Colt', 'Filly'), ('DADS', 'MOMS'), ('dudes', 'gals'), ('His', 'Her'), ('boys', 'girls'), ('COUNCILMAN', 'COUNCILWOMAN'), ('father', 'mother'), ('fatherhood', 'motherhood'), ('colt', 'filly'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('dads', 'moms'), ('his', 'her'), ('son', 'daughter'), ('fathers', 'mothers'), ('Son', 'Daughter'), ('brother', 'sister'), ('He', 'She'), ('Boys', 'Girls'), ('he', 'she'), ('King', 'Queen'), ('Twin_Brother', 'Twin_Sister'), ('TESTOSTERONE', 'ESTROGEN'), ('Spokesman', 'Spokeswoman'), ('boy', 'girl'), ('prince', 'princess'), ('Catholic_Priest', 'Nun'), ('Dad', 'Mom'), ('Businessman', 'Businesswoman'), ('king', 'queen'), ('males', 'females'), ('FATHERS', 'MOTHERS'), ('Brother', 'Sister'), ('Grandsons', 'Granddaughters'), ('sons', 'daughters'), ('WIVES', 'HUSBANDS'), ('ex_girlfriend', 'ex_boyfriend'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('UNCLE', 'AUNT'), ('grandpa', 'grandma'), ('BROTHER', 'SISTER'), ('MEN', 'WOMEN'), ('GENTLEMAN', 'LADY'), ('Man', 'Woman'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('FATHERHOOD', 'MOTHERHOOD'), ('gentleman', 'lady'), ('Testosterone', 'Estrogen'), ('MONASTERY', 'CONVENT'), ('GRANDSON', 'GRANDDAUGHTER'), ('GRANDFATHER', 'GRANDMOTHER'), ('Congressman', 'Congresswoman')}\n",
      "27014 words of dimension 300 : the, and, of, to, ..., circumscribed, whos, salvaging, anion\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(hard_embedding_file):\n",
    "    E_hard = WordEmbedding(hard_embedding_file)\n",
    "\n",
    "else:\n",
    "    with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "        defs = json.load(f)\n",
    "        print(\"def pairs size:\", len(defs))\n",
    "\n",
    "    with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "        equalize_pairs = json.load(f)\n",
    "\n",
    "    with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "        gender_specific_words = json.load(f)\n",
    "        \n",
    "    E_hard = copy.deepcopy(E)    \n",
    "    \n",
    "    # If not all def pairs present\n",
    "    for def_pair in defs:\n",
    "        if not def_pair[0] in E_hard.words or not def_pair[1] in E_hard.words:\n",
    "            defs.remove(def_pair)\n",
    "    print(\"defs pair size:\", len(defs))\n",
    "    \n",
    "    hard_debias(E_hard, gender_specific_words, defs, equalize_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 40\n",
      "Processing batch 2 of 40\n",
      "Processing batch 3 of 40\n",
      "Processing batch 4 of 40\n",
      "Processing batch 5 of 40\n",
      "Processing batch 6 of 40\n",
      "Processing batch 7 of 40\n",
      "Processing batch 8 of 40\n",
      "Processing batch 9 of 40\n",
      "Processing batch 10 of 40\n",
      "Processing batch 11 of 40\n",
      "Processing batch 12 of 40\n",
      "Processing batch 13 of 40\n",
      "Processing batch 14 of 40\n",
      "Processing batch 15 of 40\n",
      "Processing batch 16 of 40\n",
      "Processing batch 17 of 40\n",
      "Processing batch 18 of 40\n",
      "Processing batch 19 of 40\n",
      "Processing batch 20 of 40\n",
      "Processing batch 21 of 40\n",
      "Processing batch 22 of 40\n",
      "Processing batch 23 of 40\n",
      "Processing batch 24 of 40\n",
      "Processing batch 25 of 40\n",
      "Processing batch 26 of 40\n",
      "Processing batch 27 of 40\n",
      "Processing batch 28 of 40\n",
      "Processing batch 29 of 40\n",
      "Processing batch 30 of 40\n",
      "Processing batch 31 of 40\n",
      "Processing batch 32 of 40\n",
      "Processing batch 33 of 40\n",
      "Processing batch 34 of 40\n",
      "Processing batch 35 of 40\n",
      "Processing batch 36 of 40\n",
      "Processing batch 37 of 40\n",
      "Processing batch 38 of 40\n",
      "Processing batch 39 of 40\n",
      "Processing batch 40 of 40\n",
      "+-------------------------------------------------------+\n",
      "|         Results for 'Hard-debiased', FastText         |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |       Score       |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  324  |     29    | 74.17924052453014 |\n",
      "| EN-RG-65      |   56  |     9     | 83.50735694621694 |\n",
      "| MSR-analogy   |  4838 |    3162   | 55.99421248449773 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for hard-debiased\n",
    "result_hard_debiased = benchmark.evaluate(E_hard, \"'Hard-debiased', FastText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Soft debiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debiaswe.debias import soft_debias\n",
    "\n",
    "# Path for soft_debiased embedding file \n",
    "soft_embedding_file = './embeddings/fasttext_wiki-news-300d-1M_soft_debiased.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ Epoch #0: 2370557.5\n",
      "Loss @ Epoch #100: 230118.8125\n",
      "Loss @ Epoch #200: 118769.8828125\n",
      "Loss @ Epoch #300: 70960.890625\n",
      "Loss @ Epoch #400: 46132.4375\n",
      "Loss @ Epoch #500: 31855.505859375\n",
      "Loss @ Epoch #600: 23072.251953125\n",
      "Loss @ Epoch #700: 17380.1953125\n",
      "Loss @ Epoch #800: 13527.67578125\n",
      "Loss @ Epoch #900: 10818.865234375\n",
      "Loss @ Epoch #1000: 8847.076171875\n",
      "Loss @ Epoch #1100: 7365.91943359375\n",
      "Loss @ Epoch #1200: 6221.6064453125\n",
      "Loss @ Epoch #1300: 5315.6474609375\n",
      "Loss @ Epoch #1400: 4583.33251953125\n",
      "Loss @ Epoch #1500: 3980.914306640625\n",
      "Loss @ Epoch #1600: 3477.994140625\n",
      "Loss @ Epoch #1700: 3052.818115234375\n",
      "Loss @ Epoch #1800: 2689.550537109375\n",
      "Loss @ Epoch #1900: 2376.046142578125\n",
      "Loss @ Epoch #2000: 2103.907958984375\n",
      "Loss @ Epoch #2100: 1865.5926513671875\n",
      "Loss @ Epoch #2200: 1656.3790283203125\n",
      "Loss @ Epoch #2300: 1471.3201904296875\n",
      "Loss @ Epoch #2400: 1318.3330078125\n",
      "Loss @ Epoch #2500: 1222.083251953125\n",
      "Loss @ Epoch #2600: 1038.669921875\n",
      "Loss @ Epoch #2700: 921.4835815429688\n",
      "Loss @ Epoch #2800: 842.6682739257812\n",
      "Loss @ Epoch #2900: 743.4783935546875\n",
      "Loss @ Epoch #3000: 666.8333740234375\n",
      "Loss @ Epoch #3100: 600.2714233398438\n",
      "Loss @ Epoch #3200: 554.6820068359375\n",
      "Loss @ Epoch #3300: 495.2210693359375\n",
      "Loss @ Epoch #3400: 436.1525573730469\n",
      "Loss @ Epoch #3500: 410.09759521484375\n",
      "Loss @ Epoch #3600: 749.8540649414062\n",
      "Loss @ Epoch #3700: 310.9862365722656\n",
      "Loss @ Epoch #3800: 268.04217529296875\n",
      "Loss @ Epoch #3900: 258.93572998046875\n",
      "Loss @ Epoch #4000: 234.07855224609375\n",
      "Loss @ Epoch #4100: 234.92552185058594\n",
      "Loss @ Epoch #4200: 204.6851806640625\n",
      "Loss @ Epoch #4300: 205.29440307617188\n",
      "Loss @ Epoch #4400: 189.92445373535156\n",
      "Loss @ Epoch #4500: 193.2894744873047\n",
      "Loss @ Epoch #4600: 170.21505737304688\n",
      "Loss @ Epoch #4700: 155.82801818847656\n",
      "Loss @ Epoch #4800: 146.918212890625\n",
      "Loss @ Epoch #4900: 145.3057403564453\n",
      "Lowest loss: 132.42172241210938\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(hard_embedding_file):\n",
    "    E_soft = WordEmbedding(soft_embedding_file)\n",
    "else:\n",
    "    E_soft = copy.deepcopy(E)  \n",
    "    soft_debias(E_soft, gender_specific_words, defs, log=True, epochs = 5000, lr=0.01, decrease_times=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 40\n",
      "Processing batch 2 of 40\n",
      "Processing batch 3 of 40\n",
      "Processing batch 4 of 40\n",
      "Processing batch 5 of 40\n",
      "Processing batch 6 of 40\n",
      "Processing batch 7 of 40\n",
      "Processing batch 8 of 40\n",
      "Processing batch 9 of 40\n",
      "Processing batch 10 of 40\n",
      "Processing batch 11 of 40\n",
      "Processing batch 12 of 40\n",
      "Processing batch 13 of 40\n",
      "Processing batch 14 of 40\n",
      "Processing batch 15 of 40\n",
      "Processing batch 16 of 40\n",
      "Processing batch 17 of 40\n",
      "Processing batch 18 of 40\n",
      "Processing batch 19 of 40\n",
      "Processing batch 20 of 40\n",
      "Processing batch 21 of 40\n",
      "Processing batch 22 of 40\n",
      "Processing batch 23 of 40\n",
      "Processing batch 24 of 40\n",
      "Processing batch 25 of 40\n",
      "Processing batch 26 of 40\n",
      "Processing batch 27 of 40\n",
      "Processing batch 28 of 40\n",
      "Processing batch 29 of 40\n",
      "Processing batch 30 of 40\n",
      "Processing batch 31 of 40\n",
      "Processing batch 32 of 40\n",
      "Processing batch 33 of 40\n",
      "Processing batch 34 of 40\n",
      "Processing batch 35 of 40\n",
      "Processing batch 36 of 40\n",
      "Processing batch 37 of 40\n",
      "Processing batch 38 of 40\n",
      "Processing batch 39 of 40\n",
      "Processing batch 40 of 40\n",
      "+-------------------------------------------------------+\n",
      "|         Results for 'Soft-debiased', FastText         |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |       Score       |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-WS-353-ALL |  324  |     29    | 73.82362901883974 |\n",
      "| EN-RG-65      |   56  |     9     | 84.49852190812925 |\n",
      "| MSR-analogy   |  4838 |    3162   | 54.42331541959488 |\n",
      "+---------------+-------+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for soft-debiased\n",
    "result_soft_debiased = benchmark.evaluate(E_soft, \"'Soft-debiased', FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|                        Results for FastText dataset                       |\n",
      "+---------------+-------------------+-------------------+-------------------+\n",
      "|     Score     |      EN-RG-65     |   EN-WS-353-ALL   |    MSR-analogy    |\n",
      "+---------------+-------------------+-------------------+-------------------+\n",
      "|     Before    | 83.86622701863348 | 74.10786418997199 | 55.87019429516329 |\n",
      "| Hard-debiased | 83.50735694621694 | 74.17924052453014 | 55.99421248449773 |\n",
      "| Soft-debiased | 84.49852190812925 | 73.82362901883974 | 54.42331541959488 |\n",
      "+---------------+-------------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "benchmark.pprint_compare([result_original, result_hard_debiased, result_soft_debiased], [\"Before\", \"Hard-debiased\", \"Soft-debiased\"], \"FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
