{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "<debiaswe.we.WordEmbedding object at 0x7faff60767d0>\n",
      "Words: 26423\n",
      "Accuracy:  46.79681576952237 %\n",
      "Accuracy determined over 5276 queries ( 2724 queries contained OOV words)\n"
     ]
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "print(E)\n",
    "words = E.words\n",
    "print(\"Words:\", len(words))\n",
    "\n",
    "analogy_answers = np.genfromtxt(\"./benchmarks/MSR-analogy/test_set/word_relationship.answers\", dtype='str', encoding='utf-8')\n",
    "analogy_answers = np.expand_dims(analogy_answers[:,1], axis=1)\n",
    "analogy_questions = np.genfromtxt(\"./benchmarks/MSR-analogy/test_set/word_relationship.questions\", dtype='str', encoding='utf-8')\n",
    "present_words = np.isin(np.hstack((analogy_answers, analogy_questions)), E.words).all(axis=1)\n",
    "filtered_answers = analogy_answers[present_words]\n",
    "filtered_questions = analogy_questions[present_words]\n",
    "a = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,0])]\n",
    "x = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,1])]\n",
    "b = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,2])]\n",
    "all_y = E.vecs\n",
    "y_scores = (((1+all_y@x.T)/2)*((1+all_y@b.T)/2))/((1+all_y@a.T+0.00000001)/2)\n",
    "query_word_indices = np.vectorize(E.index.__getitem__)(filtered_questions).T\n",
    "#y_scores[query_word_indices, np.arange(y_scores.shape[1])[None,:]] = 0\n",
    "y = np.expand_dims(np.array(E.words)[np.argmax(y_scores, axis=0)], axis=1)\n",
    "score = np.mean(y==filtered_answers)\n",
    "print(\"Accuracy: \",score*100, \"%\")\n",
    "words_not_found = len(analogy_answers) - len(filtered_answers)\n",
    "print(\"Accuracy determined over\", len(filtered_answers), \"queries (\", words_not_found, \"queries contained OOV words)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitional [['woman', 'man'], ['girl', 'boy'], ['she', 'he'], ['mother', 'father'], ['daughter', 'son'], ['gal', 'guy'], ['female', 'male'], ['her', 'his'], ['herself', 'himself'], ['Mary', 'John']]\n",
      "gender specific 218 ['actress', 'actresses', 'aunt', 'aunts', 'bachelor', 'ballerina', 'barbershop', 'baritone', 'beard', 'beards']\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "{('Congressman', 'Congresswoman'), ('DAD', 'MOM'), ('his', 'her'), ('Grandpa', 'Grandma'), ('kings', 'queens'), ('grandfather', 'grandmother'), ('king', 'queen'), ('NEPHEW', 'NIECE'), ('Wives', 'Husbands'), ('KING', 'QUEEN'), ('Councilman', 'Councilwoman'), ('BOYS', 'GIRLS'), ('HIMSELF', 'HERSELF'), ('twin_brother', 'twin_sister'), ('Prince', 'Princess'), ('Male', 'Female'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Man', 'Woman'), ('grandsons', 'granddaughters'), ('GRANDSON', 'GRANDDAUGHTER'), ('chairman', 'chairwoman'), ('HE', 'SHE'), ('DUDES', 'GALS'), ('Men', 'Women'), ('Kings', 'Queens'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('WIVES', 'HUSBANDS'), ('GRANDFATHER', 'GRANDMOTHER'), ('Nephew', 'Niece'), ('MONASTERY', 'CONVENT'), ('MEN', 'WOMEN'), ('Businessman', 'Businesswoman'), ('father', 'mother'), ('fraternity', 'sorority'), ('Fatherhood', 'Motherhood'), ('MAN', 'WOMAN'), ('GELDING', 'MARE'), ('congressman', 'congresswoman'), ('COUNCILMAN', 'COUNCILWOMAN'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('prostate_cancer', 'ovarian_cancer'), ('Himself', 'Herself'), ('grandpa', 'grandma'), ('Dudes', 'Gals'), ('Testosterone', 'Estrogen'), ('Gelding', 'Mare'), ('monastery', 'convent'), ('MALES', 'FEMALES'), ('Twin_Brother', 'Twin_Sister'), ('male', 'female'), ('UNCLE', 'AUNT'), ('spokesman', 'spokeswoman'), ('colt', 'filly'), ('MALE', 'FEMALE'), ('son', 'daughter'), ('SONS', 'DAUGHTERS'), ('FATHERHOOD', 'MOTHERHOOD'), ('BROTHER', 'SISTER'), ('schoolboy', 'schoolgirl'), ('boys', 'girls'), ('PRINCE', 'PRINCESS'), ('dad', 'mom'), ('Monastery', 'Convent'), ('Catholic_Priest', 'Nun'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Fraternity', 'Sorority'), ('Gentleman', 'Lady'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('dads', 'moms'), ('Uncle', 'Aunt'), ('Father', 'Mother'), ('Son', 'Daughter'), ('testosterone', 'estrogen'), ('CHAIRMAN', 'CHAIRWOMAN'), ('FELLA', 'GRANNY'), ('HIS', 'HER'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('KINGS', 'QUEENS'), ('nephew', 'niece'), ('FATHERS', 'MOTHERS'), ('TESTOSTERONE', 'ESTROGEN'), ('CATHOLIC_PRIEST', 'NUN'), ('fathers', 'mothers'), ('men', 'women'), ('Brothers', 'Sisters'), ('gentleman', 'lady'), ('dudes', 'gals'), ('His', 'Her'), ('Spokesman', 'Spokeswoman'), ('businessman', 'businesswoman'), ('COLT', 'FILLY'), ('Grandsons', 'Granddaughters'), ('Brother', 'Sister'), ('males', 'females'), ('FRATERNITY', 'SORORITY'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('Boy', 'Girl'), ('BROTHERS', 'SISTERS'), ('brother', 'sister'), ('GRANDPA', 'GRANDMA'), ('GENTLEMAN', 'LADY'), ('man', 'woman'), ('Fathers', 'Mothers'), ('grandson', 'granddaughter'), ('Schoolboy', 'Schoolgirl'), ('fella', 'granny'), ('Sons', 'Daughters'), ('himself', 'herself'), ('uncle', 'aunt'), ('brothers', 'sisters'), ('Gentlemen', 'Ladies'), ('councilman', 'councilwoman'), ('Dads', 'Moms'), ('he', 'she'), ('prince', 'princess'), ('FATHER', 'MOTHER'), ('wives', 'husbands'), ('boy', 'girl'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('Dad', 'Mom'), ('Boys', 'Girls'), ('GENTLEMEN', 'LADIES'), ('Grandfather', 'Grandmother'), ('King', 'Queen'), ('Fella', 'Granny'), ('He', 'She'), ('gelding', 'mare'), ('gentlemen', 'ladies'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('fatherhood', 'motherhood'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Colt', 'Filly'), ('BOY', 'GIRL'), ('Chairman', 'Chairwoman'), ('SPOKESMAN', 'SPOKESWOMAN'), ('sons', 'daughters'), ('catholic_priest', 'nun')}\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "Accuracy:  46.967399545109934 %\n"
     ]
    }
   ],
   "source": [
    "from debiaswe.debias import debias\n",
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f)\n",
    "print(\"definitional\", defs)\n",
    "\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "    gender_specific_words = json.load(f)\n",
    "print(\"gender specific\", len(gender_specific_words), gender_specific_words[:10])\n",
    "debias(E, gender_specific_words, defs, equalize_pairs)\n",
    "\n",
    "a = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,0])]\n",
    "x = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,1])]\n",
    "b = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,2])]\n",
    "all_y = E.vecs\n",
    "y_scores = (((1+all_y@x.T)/2)*((1+all_y@b.T)/2))/((1+all_y@a.T+0.00000001)/2)\n",
    "query_word_indices = np.vectorize(E.index.__getitem__)(filtered_questions).T\n",
    "#y_scores[query_word_indices, np.arange(y_scores.shape[1])[None,:]] = 0\n",
    "y = np.expand_dims(np.array(E.words)[np.argmax(y_scores, axis=0)], axis=1)\n",
    "score = np.mean(y==filtered_answers)\n",
    "print(\"Accuracy: \",score*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "(46.79681576952237, 2724)\n"
     ]
    }
   ],
   "source": [
    "from debiaswe.benchmarks import MSR\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "print(MSR(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
