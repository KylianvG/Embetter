{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "<debiaswe.we.WordEmbedding object at 0x1a3067dcd0>\n",
      "Words: 26423\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "./benchmarks/MSR-analogy/test_set/word_relationship.answers not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b8d551eb427b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0manalogy_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./benchmarks/MSR-analogy/test_set/word_relationship.answers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0manalogy_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalogy_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0manalogy_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./benchmarks/MSR-analogy/test_set/word_relationship.questions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1742\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m             \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    622\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ./benchmarks/MSR-analogy/test_set/word_relationship.answers not found."
     ]
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "print(E)\n",
    "words = E.words\n",
    "print(\"Words:\", len(words))\n",
    "\n",
    "analogy_answers = np.genfromtxt(\"./benchmarks/MSR-analogy/test_set/word_relationship.answers\", dtype='str', encoding='utf-8')\n",
    "analogy_answers = np.expand_dims(analogy_answers[:,1], axis=1)\n",
    "analogy_questions = np.genfromtxt(\"./benchmarks/MSR-analogy/test_set/word_relationship.questions\", dtype='str', encoding='utf-8')\n",
    "present_words = np.isin(np.hstack((analogy_answers, analogy_questions)), E.words).all(axis=1)\n",
    "filtered_answers = analogy_answers[present_words]\n",
    "filtered_questions = analogy_questions[present_words]\n",
    "a = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,0])]\n",
    "x = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,1])]\n",
    "b = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,2])]\n",
    "all_y = E.vecs\n",
    "y_scores = (((1+all_y@x.T)/2)*((1+all_y@b.T)/2))/((1+all_y@a.T+0.00000001)/2)\n",
    "query_word_indices = np.vectorize(E.index.__getitem__)(filtered_questions).T\n",
    "#y_scores[query_word_indices, np.arange(y_scores.shape[1])[None,:]] = 0\n",
    "y = np.expand_dims(np.array(E.words)[np.argmax(y_scores, axis=0)], axis=1)\n",
    "score = np.mean(y==filtered_answers)\n",
    "print(\"Accuracy: \",score*100, \"%\")\n",
    "words_not_found = len(analogy_answers) - len(filtered_answers)\n",
    "print(\"Accuracy determined over\", len(filtered_answers), \"queries (\", words_not_found, \"queries contained OOV words)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitional [['woman', 'man'], ['girl', 'boy'], ['she', 'he'], ['mother', 'father'], ['daughter', 'son'], ['gal', 'guy'], ['female', 'male'], ['her', 'his'], ['herself', 'himself'], ['Mary', 'John']]\n",
      "gender specific 218 ['actress', 'actresses', 'aunt', 'aunts', 'bachelor', 'ballerina', 'barbershop', 'baritone', 'beard', 'beards']\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "{('Ex_Girlfriend', 'Ex_Boyfriend'), ('Catholic_Priest', 'Nun'), ('MALE', 'FEMALE'), ('Man', 'Woman'), ('sons', 'daughters'), ('he', 'she'), ('twin_brother', 'twin_sister'), ('WIVES', 'HUSBANDS'), ('councilman', 'councilwoman'), ('males', 'females'), ('fraternity', 'sorority'), ('Monastery', 'Convent'), ('Schoolboy', 'Schoolgirl'), ('KING', 'QUEEN'), ('boys', 'girls'), ('GELDING', 'MARE'), ('FATHERS', 'MOTHERS'), ('CHAIRMAN', 'CHAIRWOMAN'), ('catholic_priest', 'nun'), ('Fraternity', 'Sorority'), ('HE', 'SHE'), ('nephew', 'niece'), ('Chairman', 'Chairwoman'), ('Businessman', 'Businesswoman'), ('Gentleman', 'Lady'), ('FELLA', 'GRANNY'), ('GRANDFATHER', 'GRANDMOTHER'), ('BROTHER', 'SISTER'), ('brother', 'sister'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Nephew', 'Niece'), ('prince', 'princess'), ('ex_girlfriend', 'ex_boyfriend'), ('Grandson', 'Granddaughter'), ('colt', 'filly'), ('men', 'women'), ('wives', 'husbands'), ('NEPHEW', 'NIECE'), ('Grandfather', 'Grandmother'), ('Brother', 'Sister'), ('fella', 'granny'), ('BROTHERS', 'SISTERS'), ('Fella', 'Granny'), ('BOYS', 'GIRLS'), ('Himself', 'Herself'), ('dudes', 'gals'), ('Twin_Brother', 'Twin_Sister'), ('Colt', 'Filly'), ('Wives', 'Husbands'), ('his', 'her'), ('DUDES', 'GALS'), ('testosterone', 'estrogen'), ('Dudes', 'Gals'), ('Congressman', 'Congresswoman'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('MONASTERY', 'CONVENT'), ('He', 'She'), ('gentlemen', 'ladies'), ('SONS', 'DAUGHTERS'), ('GRANDPA', 'GRANDMA'), ('gelding', 'mare'), ('King', 'Queen'), ('chairman', 'chairwoman'), ('GENTLEMEN', 'LADIES'), ('Uncle', 'Aunt'), ('congressman', 'congresswoman'), ('FRATERNITY', 'SORORITY'), ('grandsons', 'granddaughters'), ('DADS', 'MOMS'), ('UNCLE', 'AUNT'), ('MALES', 'FEMALES'), ('MEN', 'WOMEN'), ('fathers', 'mothers'), ('uncle', 'aunt'), ('king', 'queen'), ('COUNCILMAN', 'COUNCILWOMAN'), ('Fathers', 'Mothers'), ('Fatherhood', 'Motherhood'), ('KINGS', 'QUEENS'), ('CATHOLIC_PRIEST', 'NUN'), ('Gentlemen', 'Ladies'), ('boy', 'girl'), ('Male', 'Female'), ('Men', 'Women'), ('himself', 'herself'), ('Spokesman', 'Spokeswoman'), ('Testosterone', 'Estrogen'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('grandson', 'granddaughter'), ('son', 'daughter'), ('male', 'female'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('GENTLEMAN', 'LADY'), ('grandfather', 'grandmother'), ('HIMSELF', 'HERSELF'), ('gentleman', 'lady'), ('HIS', 'HER'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Brothers', 'Sisters'), ('dads', 'moms'), ('Sons', 'Daughters'), ('COLT', 'FILLY'), ('FATHERHOOD', 'MOTHERHOOD'), ('Grandpa', 'Grandma'), ('father', 'mother'), ('monastery', 'convent'), ('DAD', 'MOM'), ('grandpa', 'grandma'), ('Grandsons', 'Granddaughters'), ('prostate_cancer', 'ovarian_cancer'), ('GRANDSON', 'GRANDDAUGHTER'), ('Prince', 'Princess'), ('BOY', 'GIRL'), ('Boys', 'Girls'), ('PRINCE', 'PRINCESS'), ('Gelding', 'Mare'), ('Father', 'Mother'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('fatherhood', 'motherhood'), ('Males', 'Females'), ('spokesman', 'spokeswoman'), ('kings', 'queens'), ('His', 'Her'), ('Dad', 'Mom'), ('man', 'woman'), ('TESTOSTERONE', 'ESTROGEN'), ('SON', 'DAUGHTER'), ('dad', 'mom'), ('businessman', 'businesswoman'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('Dads', 'Moms'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Kings', 'Queens'), ('schoolboy', 'schoolgirl'), ('Son', 'Daughter'), ('Boy', 'Girl'), ('brothers', 'sisters'), ('Councilman', 'Councilwoman'), ('MAN', 'WOMAN'), ('FATHER', 'MOTHER')}\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filtered_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3a6fdd870235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhard_debias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_specific_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequalize_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_questions' is not defined"
     ]
    }
   ],
   "source": [
    "from debiaswe.debias import hard_debias\n",
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f)\n",
    "print(\"definitional\", defs)\n",
    "\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "    gender_specific_words = json.load(f)\n",
    "print(\"gender specific\", len(gender_specific_words), gender_specific_words[:10])\n",
    "hard_debias(E, gender_specific_words, defs, equalize_pairs)\n",
    "\n",
    "a = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,0])]\n",
    "x = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,1])]\n",
    "b = E.vecs[np.vectorize(E.index.__getitem__)(filtered_questions[:,2])]\n",
    "all_y = E.vecs\n",
    "y_scores = (((1+all_y@x.T)/2)*((1+all_y@b.T)/2))/((1+all_y@a.T+0.00000001)/2)\n",
    "query_word_indices = np.vectorize(E.index.__getitem__)(filtered_questions).T\n",
    "#y_scores[query_word_indices, np.arange(y_scores.shape[1])[None,:]] = 0\n",
    "y = np.expand_dims(np.array(E.words)[np.argmax(y_scores, axis=0)], axis=1)\n",
    "score = np.mean(y==filtered_answers)\n",
    "print(\"Accuracy: \",score*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "+-------------------------------------------------------+\n",
      "|                    Results for test                   |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |    Score (rho)    |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-RG-65      |   53  |     12    | 77.66555804950227 |\n",
      "| EN-WS-353-ALL |  318  |     35    | 68.82719646959825 |\n",
      "| MSR-analogy   |  5276 |    2724   | 46.79681576952237 |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "{'EN-RG-65': [53, 12, 77.66555804950227], 'EN-WS-353-ALL': [318, 35, 68.82719646959825], 'MSR-analogy': [5276, 2724, 46.79681576952237]}\n"
     ]
    }
   ],
   "source": [
    "from debiaswe.benchmarks import Benchmark\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "B = Benchmark()\n",
    "print(B.evaluate(E, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
