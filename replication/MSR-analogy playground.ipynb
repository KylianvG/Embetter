{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n"
     ]
    }
   ],
   "source": [
    "from debiaswe.benchmarks import Benchmark\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "+-------------------------------------------------------+\n",
      "|                    Results for test                   |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| Dataset       | Found | Not Found |    Score (rho)    |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "| EN-RG-65      |   53  |     12    | 77.66555804950227 |\n",
      "| EN-WS-353-ALL |  318  |     35    | 68.82719646959825 |\n",
      "| MSR-analogy   |  5276 |    2724   | 46.79681576952237 |\n",
      "+---------------+-------+-----------+-------------------+\n",
      "{'EN-RG-65': [53, 12, 77.66555804950227], 'EN-WS-353-ALL': [318, 35, 68.82719646959825], 'MSR-analogy': [5276, 2724, 46.79681576952237]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B = Benchmark()\n",
    "print(B.evaluate(E, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded professions\n",
      "Format:\n",
      "word,\n",
      "definitional female -1.0 -> definitional male 1.0\n",
      "stereotypical female -1.0 -> stereotypical male 1.0\n"
     ]
    }
   ],
   "source": [
    "professions = load_professions()\n",
    "profession_words = [p[0] for p in professions]\n",
    "with open('./data/definitional_pairs.json', \"r\") as f:\n",
    "    defs = json.load(f)\n",
    "unzipped_defs = list(zip(*defs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_defs = np.array(unzipped_defs[0])\n",
    "male_defs = np.array(unzipped_defs[1])\n",
    "A = E.vecs[np.vectorize(E.index.__getitem__)(female_defs)]\n",
    "B = E.vecs[np.vectorize(E.index.__getitem__)(male_defs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_gender = we.doPCA(defs, E).components_[0]\n",
    "sp = sorted([(E.v(w).dot(v_gender), w) for w in profession_words])\n",
    "unzipped_sp = list(zip(*sp))\n",
    "prof_scores = np.array(unzipped_sp[0])\n",
    "sorted_profs = np.array(unzipped_sp[1])\n",
    "female_prof = sorted_profs[prof_scores>0]\n",
    "male_prof = sorted_profs[prof_scores<0]\n",
    "\n",
    "def balance_word_vectors(A, B):\n",
    "    \"\"\"\n",
    "    Balance size of two lists of word vectors by randomly deleting some vectors in larger one.\n",
    "    :param A: (len(words), dim) shaped numpy ndarrary of word vectors\n",
    "    :param B: (len(words), dim) shaped numpy ndarrary of word vectors\n",
    "    :return: tuple of two balanced word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    diff = len(A) - len(B)\n",
    "\n",
    "    if diff > 0:\n",
    "        A = np.delete(A, np.random.choice(len(A), diff, 0), axis=0)\n",
    "    else:\n",
    "        B = np.delete(B, np.random.choice(len(B), -diff, 0), axis=0)\n",
    "\n",
    "    return A, B\n",
    "\n",
    "female_prof, male_prof = balance_word_vectors(female_prof, male_prof)\n",
    "\n",
    "X = E.vecs[np.vectorize(E.index.__getitem__)(np.array(female_prof))]\n",
    "Y = E.vecs[np.vectorize(E.index.__getitem__)(np.array(male_prof))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4822595\n"
     ]
    }
   ],
   "source": [
    "x_association = np.mean((X @ A.T), axis=-1) - np.mean((X @ B.T), axis=-1)\n",
    "y_association = np.mean((Y @ A.T), axis=-1) - np.mean((Y @ B.T), axis=-1)\n",
    "\n",
    "tmp1 = np.mean(x_association, axis=-1) - np.mean(y_association, axis=-1)\n",
    "tmp2 = np.std(np.concatenate((x_association, y_association), axis=0))\n",
    "\n",
    "print(tmp1/tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "{('ex_girlfriend', 'ex_boyfriend'), ('councilman', 'councilwoman'), ('father', 'mother'), ('son', 'daughter'), ('COLT', 'FILLY'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('FATHER', 'MOTHER'), ('his', 'her'), ('FRATERNITY', 'SORORITY'), ('SPOKESMAN', 'SPOKESWOMAN'), ('WIVES', 'HUSBANDS'), ('BROTHER', 'SISTER'), ('MEN', 'WOMEN'), ('KINGS', 'QUEENS'), ('Fatherhood', 'Motherhood'), ('kings', 'queens'), ('Councilman', 'Councilwoman'), ('Nephew', 'Niece'), ('grandfather', 'grandmother'), ('boy', 'girl'), ('Dads', 'Moms'), ('CATHOLIC_PRIEST', 'NUN'), ('FATHERS', 'MOTHERS'), ('FELLA', 'GRANNY'), ('brothers', 'sisters'), ('Colt', 'Filly'), ('prostate_cancer', 'ovarian_cancer'), ('wives', 'husbands'), ('Dudes', 'Gals'), ('HE', 'SHE'), ('congressman', 'congresswoman'), ('Testosterone', 'Estrogen'), ('boys', 'girls'), ('DAD', 'MOM'), ('grandson', 'granddaughter'), ('dad', 'mom'), ('Dad', 'Mom'), ('brother', 'sister'), ('gelding', 'mare'), ('King', 'Queen'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('prince', 'princess'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('monastery', 'convent'), ('Prince', 'Princess'), ('MONASTERY', 'CONVENT'), ('king', 'queen'), ('MALE', 'FEMALE'), ('Uncle', 'Aunt'), ('Grandson', 'Granddaughter'), ('Spokesman', 'Spokeswoman'), ('himself', 'herself'), ('Grandpa', 'Grandma'), ('MAN', 'WOMAN'), ('Himself', 'Herself'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('TESTOSTERONE', 'ESTROGEN'), ('Male', 'Female'), ('Fella', 'Granny'), ('Grandfather', 'Grandmother'), ('HIS', 'HER'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Men', 'Women'), ('Wives', 'Husbands'), ('MALES', 'FEMALES'), ('Catholic_Priest', 'Nun'), ('grandpa', 'grandma'), ('fathers', 'mothers'), ('gentleman', 'lady'), ('gentlemen', 'ladies'), ('GRANDFATHER', 'GRANDMOTHER'), ('Boys', 'Girls'), ('His', 'Her'), ('Kings', 'Queens'), ('Twin_Brother', 'Twin_Sister'), ('Sons', 'Daughters'), ('twin_brother', 'twin_sister'), ('Father', 'Mother'), ('KING', 'QUEEN'), ('DADS', 'MOMS'), ('catholic_priest', 'nun'), ('colt', 'filly'), ('men', 'women'), ('man', 'woman'), ('CHAIRMAN', 'CHAIRWOMAN'), ('male', 'female'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('GENTLEMEN', 'LADIES'), ('Brothers', 'Sisters'), ('SONS', 'DAUGHTERS'), ('males', 'females'), ('FATHERHOOD', 'MOTHERHOOD'), ('Grandsons', 'Granddaughters'), ('HIMSELF', 'HERSELF'), ('nephew', 'niece'), ('schoolboy', 'schoolgirl'), ('Gentleman', 'Lady'), ('Businessman', 'Businesswoman'), ('uncle', 'aunt'), ('chairman', 'chairwoman'), ('dads', 'moms'), ('testosterone', 'estrogen'), ('spokesman', 'spokeswoman'), ('Boy', 'Girl'), ('He', 'She'), ('grandsons', 'granddaughters'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('Brother', 'Sister'), ('NEPHEW', 'NIECE'), ('GRANDPA', 'GRANDMA'), ('fella', 'granny'), ('BROTHERS', 'SISTERS'), ('BOYS', 'GIRLS'), ('GELDING', 'MARE'), ('COUNCILMAN', 'COUNCILWOMAN'), ('Chairman', 'Chairwoman'), ('BOY', 'GIRL'), ('fraternity', 'sorority'), ('GRANDSON', 'GRANDDAUGHTER'), ('SON', 'DAUGHTER'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('he', 'she'), ('Males', 'Females'), ('Man', 'Woman'), ('Fathers', 'Mothers'), ('fatherhood', 'motherhood'), ('UNCLE', 'AUNT'), ('sons', 'daughters'), ('DUDES', 'GALS'), ('Congressman', 'Congresswoman'), ('Monastery', 'Convent'), ('Fraternity', 'Sorority'), ('Gelding', 'Mare'), ('GENTLEMAN', 'LADY'), ('Gentlemen', 'Ladies'), ('PRINCE', 'PRINCESS'), ('Son', 'Daughter')}\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "0.28041717\n"
     ]
    }
   ],
   "source": [
    "from debiaswe.debias import hard_debias\n",
    "with open('./data/equalize_pairs.json', \"r\") as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
    "    gender_specific_words = json.load(f)\n",
    "hard_debias(E, gender_specific_words, defs, equalize_pairs)\n",
    "A = E.vecs[np.vectorize(E.index.__getitem__)(female_defs)]\n",
    "B = E.vecs[np.vectorize(E.index.__getitem__)(male_defs)]\n",
    "X = E.vecs[np.vectorize(E.index.__getitem__)(np.array(female_prof))]\n",
    "Y = E.vecs[np.vectorize(E.index.__getitem__)(np.array(male_prof))]\n",
    "x_association = np.mean((X @ A.T), axis=-1) - np.mean((X @ B.T), axis=-1)\n",
    "y_association = np.mean((Y @ A.T), axis=-1) - np.mean((Y @ B.T), axis=-1)\n",
    "\n",
    "tmp1 = np.mean(x_association, axis=-1) - np.mean(y_association, axis=-1)\n",
    "tmp2 = np.std(np.concatenate((x_association, y_association), axis=0))\n",
    "\n",
    "print(tmp1/tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import combinations, filterfalse\n",
    "def swAB(W, A, B):\n",
    "  \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
    "     Arguments\n",
    "              W, A, B : n x d matrix of word embeddings stored row wise\n",
    "  \"\"\"\n",
    "  WA = W @ A.T\n",
    "  WB = W @ B.T\n",
    "  \n",
    "  #Take mean along columns\n",
    "  WAmean = np.mean(WA, axis = 1)\n",
    "  WBmean = np.mean(WB, axis = 1)\n",
    "  \n",
    "  return (WAmean - WBmean)\n",
    "  \n",
    "def test_statistic(X, Y, A, B):\n",
    "  \"\"\"Calculates test-statistic between the pair of association words and target words\n",
    "     Arguments\n",
    "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
    "     Returns\n",
    "              Test Statistic\n",
    "  \"\"\"\n",
    "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n",
    "\n",
    "def random_permutation(iterable, r=None):\n",
    "  \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
    "  pool = tuple(iterable)\n",
    "  r = len(pool) if r is None else r\n",
    "  return tuple(random.sample(pool, r))\n",
    "\n",
    "def weat_p_value(X, Y, A, B, E, sample = 1000):\n",
    "  \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "              sample : Number of random permutations used.\n",
    "     Returns\n",
    "  \"\"\"\n",
    "  size_of_permutation = min(len(X), len(Y))\n",
    "  X_Y = X + Y\n",
    "  test_stats_over_permutation = []\n",
    "  \n",
    "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "  \n",
    "  if not sample:\n",
    "      permutations = combinations(X_Y, size_of_permutation)\n",
    "  else:\n",
    "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
    "      \n",
    "  for Xi in permutations:\n",
    "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
    "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
    "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
    "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
    "    \n",
    "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
    "  \n",
    "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
    "  \n",
    "  return is_over.sum() / is_over.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
