{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# BERT\n",
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "import numpy as np\n",
    "\n",
    "# XLNet\n",
    "import transformers\n",
    "\n",
    "# Paper\n",
    "import json\n",
    "import random\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(words, encodings, word_idx, measure=\"cos\"):\n",
    "    cos = CosineSimilarity(dim=0)\n",
    "    similarities = []\n",
    "    print(\"Similarity with \\'\"+words[word_idx]+\"\\'\")\n",
    "    for i in range(len(words)):\n",
    "        # cos or torch.dist\n",
    "        if measure == \"cos\":\n",
    "            sim = cos(encodings[i], encodings[word_idx])\n",
    "            print(words[i]+\"\\t\"+str(sim.item()))\n",
    "        elif measure == \"euclidean\":\n",
    "            sim = torch.dist(encodings[i], encodings[word_idx])\n",
    "            print(words[i]+\"\\t\"+str(sim.item()))\n",
    "        else:\n",
    "            print(\"Unknown distance metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 0, 103, 100, 102]\n",
      "['[CLS]', '[PAD]', '[MASK]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# bert_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "# bert_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "def w2BERT(words, tokenizer=bert_tokenizer, model=bert_model):\n",
    "    # Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n",
    "    indexed_tokens = tokenizer.encode(\" \".join(words), add_special_tokens=False)\n",
    "    for i, idx in enumerate(indexed_tokens):\n",
    "        if idx == 100:\n",
    "            print(words[i]+\" is unknown\")\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor)\n",
    "        print(encoded_layers.shape)\n",
    "    return encoded_layers[0]\n",
    "\n",
    "def w2BERT2(words, tokenizer=bert_tokenizer, model=bert_model):\n",
    "    encodings = torch.Tensor(len(words), 768)\n",
    "    for i, word in enumerate(words):\n",
    "        indexed_tokens = tokenizer.encode(word, add_special_tokens=True)\n",
    "        if 100 in indexed_tokens: print(word+\" is unknown\")\n",
    "        print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "        with torch.no_grad():\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            encoded_layers = model(tokens_tensor)[0]\n",
    "        encodings[i,:] = encoded_layers[0,0]\n",
    "    encodings = encodings / encodings.norm(dim=1)[:, None]\n",
    "    return encodings\n",
    "\n",
    "print(bert_tokenizer.all_special_ids)\n",
    "print(bert_tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'man', '[SEP]']\n",
      "(tensor([[[ 0.4628, -0.1432,  0.0398,  ..., -0.1443,  0.3387, -0.0368],\n",
      "         [ 0.7357, -0.8538,  0.2751,  ..., -0.4005,  0.3785, -0.0588],\n",
      "         [ 1.4516, -0.4821, -0.8175,  ..., -0.1294,  1.0990, -0.2170]]]), tensor([[-0.7187,  0.4139,  0.9999, -0.9945,  0.9638,  0.8383,  0.9832, -0.9723,\n",
      "         -0.9790, -0.5187,  0.9843,  0.9991, -0.9909, -0.9998,  0.6829, -0.9781,\n",
      "          0.9861, -0.5522, -1.0000, -0.6345, -0.3825, -0.9999,  0.3836,  0.9595,\n",
      "          0.9815,  0.0098,  0.9877,  1.0000,  0.8148,  0.1581,  0.3823, -0.9916,\n",
      "          0.7170, -0.9989,  0.1441,  0.1302,  0.2561, -0.2670,  0.4982, -0.9063,\n",
      "         -0.6838, -0.1601,  0.7203, -0.4586,  0.7653,  0.1111,  0.1433, -0.0395,\n",
      "         -0.2101,  0.9999, -0.9645,  0.9993, -0.9795,  0.9974,  0.9968,  0.2899,\n",
      "          0.9967,  0.1308, -0.9942,  0.4293,  0.9695,  0.1057,  0.9032, -0.3748,\n",
      "          0.0913, -0.4728, -0.7935,  0.0472, -0.3879, -0.0241,  0.2885,  0.4127,\n",
      "          0.9881, -0.8863, -0.0464, -0.9284,  0.3836, -0.9999,  0.9495,  1.0000,\n",
      "          0.5126, -0.9998,  0.9961, -0.3371, -0.6553,  0.5780, -0.9919, -0.9996,\n",
      "          0.1221, -0.7250,  0.6961, -0.9891,  0.6085, -0.6315,  1.0000, -0.6545,\n",
      "         -0.2038,  0.4404,  0.8839, -0.7593, -0.6716,  0.7702,  0.9916, -0.9659,\n",
      "          0.9944,  0.4809, -0.9160, -0.8447,  0.5658,  0.1140,  0.9866, -0.9864,\n",
      "         -0.8602,  0.1361,  0.9592, -0.8837,  0.9915,  0.7789, -0.2706,  1.0000,\n",
      "         -0.1960,  0.8956,  0.9989,  0.5008, -0.4515, -0.3001, -0.4263,  0.8080,\n",
      "         -0.3365, -0.5549,  0.8142, -0.9933, -0.9900,  0.9996, -0.2911,  1.0000,\n",
      "         -0.9994,  0.9665, -0.9999, -0.8500, -0.2139,  0.0211, -0.9501,  0.5755,\n",
      "          0.9940,  0.1071, -0.8174, -0.7059,  0.6791, -0.7086,  0.4515,  0.6285,\n",
      "         -0.9493,  0.9980,  0.9929,  0.8860,  0.9597,  0.2245, -0.8616,  0.6253,\n",
      "          0.9897, -0.9997,  0.8436, -0.9684,  0.9993,  0.9573,  0.6336, -0.9845,\n",
      "          0.9999, -0.4012,  0.1104, -0.1995,  0.0087, -0.9941,  0.4578,  0.5308,\n",
      "          0.7608,  0.9994, -0.9927,  0.9996,  0.9832,  0.1693,  0.7263,  0.9938,\n",
      "         -0.9968, -0.9785, -0.9887,  0.2066,  0.4023,  0.6697,  0.5111,  0.9652,\n",
      "          0.9896,  0.6385, -0.9975, -0.4378,  0.9815, -0.1152,  1.0000, -0.2245,\n",
      "         -0.9999, -0.5211,  0.7564,  0.9877, -0.3424,  0.9762, -0.6070, -0.1134,\n",
      "          0.9655, -0.9985,  0.9862,  0.3464,  0.5878,  0.7862,  0.9949, -0.7583,\n",
      "         -0.2011,  0.1883, -0.4938,  0.9999, -0.9997, -0.3211,  0.5484, -0.9954,\n",
      "         -0.9989,  0.9804, -0.0491, -0.6139, -0.3169,  0.1125,  0.1248,  0.8097,\n",
      "          0.9911, -0.3527, -0.4349, -0.9999, -0.9763, -0.8458, -0.8752,  0.0644,\n",
      "          0.6786, -0.4664, -0.9460, -0.9921,  0.9681,  0.6666, -0.7335,  0.2162,\n",
      "         -0.4135, -0.9902,  0.3001, -0.8043, -0.9988,  0.9996, -0.4408,  0.9827,\n",
      "          0.9752, -0.9954,  0.5698, -0.9934, -0.0216, -0.9985,  0.4830, -0.0884,\n",
      "         -0.3675,  0.0383,  0.9950, -0.9733, -0.6401,  0.7871, -0.9999,  0.9439,\n",
      "         -0.2928,  0.9995,  0.2567,  0.2873,  0.9899,  0.8972, -0.9894, -0.9999,\n",
      "          0.9295,  0.9874, -0.9947, -0.2913,  1.0000, -0.9842, -0.7235, -0.9399,\n",
      "         -0.9967, -0.9997,  0.1571, -0.6187,  0.0444,  0.9814, -0.0240,  0.0906,\n",
      "          0.9961,  0.9977,  0.1102, -0.1764,  0.0980, -0.9626, -0.9871,  0.6085,\n",
      "          0.1866, -1.0000,  0.9999, -0.9973,  0.9985,  0.8071, -0.9717,  0.8119,\n",
      "         -0.0045, -0.8966,  0.0153,  0.9999,  0.9825, -0.1564,  0.2946,  0.8824,\n",
      "         -0.2134,  0.5343, -0.7626, -0.4737,  0.2957, -0.9446,  0.9829,  0.5712,\n",
      "         -0.9930,  0.9819,  0.1359,  0.7713, -0.6395,  0.8675,  0.9917, -0.0414,\n",
      "         -0.0848, -0.2087, -0.8552, -0.9623,  0.1141, -0.9901, -0.3298,  0.9351,\n",
      "          0.9888, -0.9922,  0.9974, -0.1773,  0.8439, -0.9895,  1.0000, -0.9982,\n",
      "          0.0762,  0.6967, -0.6610,  0.0872,  0.9906,  0.9626,  0.9429, -0.9458,\n",
      "         -0.7588,  0.8469,  0.9772, -0.9217, -0.0272, -0.9954, -0.5848,  0.9942,\n",
      "          0.9829, -0.0522, -0.4639, -0.9904,  0.9707, -0.6351, -0.7495, -0.1809,\n",
      "         -0.7175,  0.6824,  0.9884, -0.5108,  0.6198,  0.1759, -0.9869,  0.6892,\n",
      "          0.8388,  0.9999, -0.9807,  0.6007,  0.9892, -0.2824, -0.6442,  0.5675,\n",
      "          0.9944, -0.9551, -0.2813, -0.9997,  0.0094, -0.5286, -0.2604, -0.3773,\n",
      "          0.1124, -0.8261,  0.9162,  0.1151,  0.8180, -0.4585,  0.9780, -0.2260,\n",
      "         -0.0897, -0.4562, -0.2227,  0.4410,  0.1547,  0.9817, -0.9680,  0.9998,\n",
      "         -0.0997, -1.0000, -0.9854, -0.7128, -0.9996,  0.2898, -0.9980,  0.9893,\n",
      "          0.8116, -0.9917, -0.9925, -0.9986, -0.9982,  0.6322,  0.3514, -0.1236,\n",
      "          0.3635,  0.7602,  0.1078, -0.4212, -0.1498, -0.9344, -0.5476, -0.9876,\n",
      "          0.7656, -1.0000, -0.7964,  0.9921, -0.9901, -0.9344, -0.9274, -0.2195,\n",
      "         -0.8444,  0.4410,  0.9866, -0.2031, -0.5045, -0.9997,  0.9922, -0.6280,\n",
      "          0.2664, -0.8400, -0.9801,  0.9998,  0.4388, -0.1127, -0.1546, -0.9990,\n",
      "          0.9713, -0.8641, -0.9071, -0.9778,  0.1904, -0.9600, -0.9999,  0.0763,\n",
      "          0.9832,  0.9975,  0.9750,  0.3337, -0.4586, -0.9675,  0.1964, -1.0000,\n",
      "          0.6994,  0.7971, -0.9859, -0.5424,  0.9958,  0.9753, -0.9434, -0.9742,\n",
      "          0.7650,  0.3646,  0.9748, -0.0968, -0.6428,  0.3264, -0.1684, -0.9924,\n",
      "         -0.9010,  0.9974, -0.9941,  0.9886,  0.9841,  0.9935, -0.2745,  0.1373,\n",
      "         -0.9527, -0.9962, -0.4542,  0.0916, -0.9999,  0.9999, -1.0000,  0.5603,\n",
      "         -0.5047,  0.6698,  0.9902, -0.1003, -0.9999, -0.9999, -0.3412, -0.0155,\n",
      "          0.9919,  0.2884,  0.2938, -0.5907, -0.4258,  0.9986, -0.6311, -0.3778,\n",
      "         -0.9886,  0.9997,  0.6702, -0.9988,  0.9864, -0.9997,  0.8533,  0.9766,\n",
      "          0.8991,  0.9694, -0.9938,  1.0000, -0.9998,  0.9983, -1.0000, -0.9916,\n",
      "          0.9999, -0.9916, -0.5598, -0.9998, -0.9917,  0.4633,  0.1496, -0.5324,\n",
      "          0.9935, -0.9999, -0.9990, -0.4268, -0.7199, -0.7647,  0.9825, -0.4972,\n",
      "          0.9954, -0.1101,  0.9550,  0.1502,  0.9939,  0.9988, -0.6467, -0.8329,\n",
      "         -0.9940,  0.9893, -0.6066,  0.3788,  0.9529, -0.0377, -0.6770,  0.3716,\n",
      "         -0.9983,  0.1890, -0.4234,  0.7788,  0.7770,  0.8579,  0.0431, -0.4131,\n",
      "         -0.1959, -0.9933,  0.4952, -0.9997,  0.9815, -0.8106,  0.0688, -0.4496,\n",
      "          0.5530, -0.9453,  0.9997,  0.9989, -0.9992,  0.0477,  0.9916, -0.6872,\n",
      "          0.9718, -0.9955,  0.0146,  0.9269, -0.6580,  0.9808,  0.2768, -0.0762,\n",
      "          0.9802, -0.9971, -0.7633, -0.6430,  0.2429, -0.0658, -0.9780,  0.1440,\n",
      "          0.9628, -0.4023, -0.9998,  0.5640, -0.9996, -0.1249,  0.9840,  0.3420,\n",
      "          0.9999, -0.6904,  0.0760,  0.0400, -0.9999, -0.9961,  0.1722, -0.2173,\n",
      "         -0.8825,  0.9940, -0.2076,  0.8111, -0.9999,  0.3308,  0.9869,  0.3596,\n",
      "          0.8253, -0.6867, -0.9770, -0.9319, -0.6980,  0.1282,  0.6245, -0.9862,\n",
      "         -0.3560, -0.8373,  1.0000, -0.9981, -0.8435, -0.9892,  0.5009,  0.7747,\n",
      "          0.4432,  0.0471, -0.7196,  0.8578, -0.8522,  0.9979, -0.9968, -0.9975,\n",
      "          0.9999,  0.1205, -0.9804, -0.1255, -0.4695,  0.2614, -0.1517,  0.5982,\n",
      "         -0.8190, -0.2076, -0.9990,  0.4020, -0.6982, -0.9948, -0.5603, -0.5001,\n",
      "         -0.9988,  0.9966,  0.9665,  1.0000, -0.9998,  0.8816,  0.1256,  0.9994,\n",
      "         -0.0220, -0.7322,  0.7051,  0.9998, -0.6446,  0.6099, -0.0608, -0.1008,\n",
      "          0.1227, -0.5182,  0.9859, -0.8568,  0.1100, -0.9875, -0.9999,  0.9999,\n",
      "         -0.0480,  0.9905,  0.1804,  0.6730, -0.8775,  0.9776, -0.9360, -0.9131,\n",
      "         -1.0000,  0.1469, -0.9992, -0.9925,  0.0690,  0.9914, -0.9997, -0.9955,\n",
      "         -0.3383, -1.0000,  0.8155, -0.9761, -0.6309, -0.9884,  0.9895, -0.1792,\n",
      "         -0.5044,  0.9789, -0.9806,  0.8984,  0.6426,  0.4866,  0.1909,  0.1854,\n",
      "         -0.7793, -0.9742, -0.7691, -0.9675,  0.7174, -0.9904, -0.6917,  0.9975,\n",
      "          0.9886, -0.9993, -0.9970,  0.9798,  0.0739,  0.9935, -0.4384, -0.9999,\n",
      "         -0.9999,  0.0533,  0.1594,  0.9968, -0.3498,  0.9978,  0.7684, -0.1752,\n",
      "          0.1845, -0.6403, -0.3807, -0.5019, -0.3095,  1.0000, -0.7227,  0.9933]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c8fddf51b76e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"man\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"woman\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"car\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tractor\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bee\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"vehicle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"motor\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"potato\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"carrot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"food\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"metal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"she\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"he\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"programmer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"computer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2BERT2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-36f6516548be>\u001b[0m in \u001b[0;36mw2BERT2\u001b[1;34m(words, tokenizer, model)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mencoded_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencodings\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "words = [\"man\", \"woman\", \"car\", \"tractor\", \"bee\", \"vehicle\", \"motor\", \"potato\", \"carrot\", \"food\", \"metal\", \"she\", \"he\", \"programmer\", \"computer\"]\n",
    "encodings = w2BERT2(words)\n",
    "print(encodings.shape)\n",
    "\n",
    "similarity(words, encodings,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 7, 5, 6, 0, 4, 8, 1]\n",
      "['<cls>', '</s>', '<eod>', '<pad>', '<mask>', '<unk>', '<sep>', '<eop>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "xlnet_tokenizer = transformers.XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_model = transformers.XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "def w2xlnet(words, tokenizer=xlnet_tokenizer, model=xlnet_model):\n",
    "    indexed_tokens = tokenizer.encode(\"<sep>\".join(words), add_special_tokens=True)[:-1] # Exclude final <cls>\n",
    "    for i, idx in enumerate(indexed_tokens):\n",
    "        if idx == 0:\n",
    "            print(words[i]+\" is unknown\")\n",
    "    print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(tokens_tensor)\n",
    "    return encoded_layers[0]\n",
    "\n",
    "def w2xlnet2(words, tokenizer=xlnet_tokenizer, model=xlnet_model):\n",
    "    encodings = torch.Tensor(len(words), 768)\n",
    "    for i, word in enumerate(words):\n",
    "        indexed_tokens = tokenizer.encode(word, add_special_tokens=True)\n",
    "        if 0 in indexed_tokens: print(word+\" is unknown\")\n",
    "        print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "        with torch.no_grad():\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            encoded_layers = model(tokens_tensor)[0]\n",
    "        encodings[i,:] = encoded_layers[0,0]\n",
    "    encodings = encodings / encodings.norm(dim=1)[:, None]\n",
    "    return encodings\n",
    "\n",
    "print(xlnet_tokenizer.all_special_ids)\n",
    "print(xlnet_tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁man', '<sep>', '<cls>']\n",
      "['▁woman', '<sep>', '<cls>']\n",
      "['▁farmer', '<sep>', '<cls>']\n",
      "['▁tractor', '<sep>', '<cls>']\n",
      "['▁flower', '<sep>', '<cls>']\n",
      "['▁car', '<sep>', '<cls>']\n",
      "['▁vehicle', '<sep>', '<cls>']\n",
      "['▁motor', '<sep>', '<cls>']\n",
      "['▁be', 'e', '<sep>', '<cls>']\n",
      "['▁potato', '<sep>', '<cls>']\n",
      "['▁carrot', '<sep>', '<cls>']\n",
      "['▁food', '<sep>', '<cls>']\n",
      "['▁metal', '<sep>', '<cls>']\n",
      "['▁she', '<sep>', '<cls>']\n",
      "['▁he', '<sep>', '<cls>']\n",
      "['▁programmer', '<sep>', '<cls>']\n",
      "['▁computer', '<sep>', '<cls>']\n",
      "torch.Size([17, 768])\n",
      "Similarity with 'tractor'\n",
      "man\t0.9632200002670288\n",
      "woman\t0.9631847739219666\n",
      "farmer\t0.9791725277900696\n",
      "tractor\t1.0\n",
      "flower\t0.9555322527885437\n",
      "car\t0.9815327525138855\n",
      "vehicle\t0.9430563449859619\n",
      "motor\t0.9505726099014282\n",
      "bee\t0.9657217264175415\n",
      "potato\t0.9759369492530823\n",
      "carrot\t0.9839041233062744\n",
      "food\t0.9652472734451294\n",
      "metal\t0.9675664305686951\n",
      "she\t0.9368990063667297\n",
      "he\t0.9590157866477966\n",
      "programmer\t0.9410752058029175\n",
      "computer\t0.9757446646690369\n"
     ]
    }
   ],
   "source": [
    "words = [\"man\", \"woman\", \"farmer\", \"tractor\", \"flower\", \"car\", \"vehicle\", \"motor\", \"bee\", \"potato\", \"carrot\", \"food\", \"metal\", \"she\", \"he\", \"programmer\", \"computer\"]\n",
    "encodings = w2xlnet2(words)\n",
    "print(encodings.shape)\n",
    "\n",
    "similarity(words, encodings,3, measure=\"cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet words to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "<debiaswe.we.WordEmbedding object at 0x0000024E298C5788>\n",
      "Words: 26423\n"
     ]
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "print(E)\n",
    "words = E.words\n",
    "print(\"Words:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w2XLNet(words, filename):\n",
    "    xlnet_tokenizer = transformers.XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    xlnet_model = transformers.XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "    unknowns = []\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(words):\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            f.write(word+\" \")\n",
    "            indexed_tokens = xlnet_tokenizer.encode(word, add_special_tokens=False)\n",
    "            if 0 in indexed_tokens:\n",
    "                print(word+\" is unknown\")\n",
    "                unknowns.append(word)\n",
    "    #       print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "            with torch.no_grad():\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                encoded_layers = xlnet_model(tokens_tensor)[0][0,0] # Tensor\n",
    "                vector_list = encoded_layers.tolist()\n",
    "                vector_list = [str(x) for x in vector_list]\n",
    "                f.write(\" \".join(vector_list)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n"
     ]
    }
   ],
   "source": [
    "save_w2XLNet(words, \"xlnet_specials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
