{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# BERT\n",
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "import numpy as np\n",
    "\n",
    "# XLNet\n",
    "import transformers\n",
    "\n",
    "# Paper\n",
    "import json\n",
    "import random\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(words, encodings, word_idx, measure=\"cos\"):\n",
    "    cos = CosineSimilarity(dim=0)\n",
    "    similarities = []\n",
    "    sims = []\n",
    "    print(\"Similarity with \\'\"+words[word_idx]+\"\\'\")\n",
    "    for i in range(len(words)):\n",
    "        # cos or torch.dist\n",
    "        sim = None\n",
    "        if measure == \"cos\":\n",
    "            sim = cos(encodings[i], encodings[word_idx])\n",
    "        elif measure == \"euclidean\":\n",
    "            sim = torch.dist(encodings[i], encodings[word_idx])\n",
    "        else:\n",
    "            print(\"Unknown distance metric\")\n",
    "        sims.append((words[i], sim.item()))\n",
    "#         print(words[i]+\"\\t\"+str(sim.item()))\n",
    "    sims = sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "    for w, s in sims:\n",
    "        print(w+\"\\t\"+str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 0, 103, 100, 102]\n",
      "['[CLS]', '[PAD]', '[MASK]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# bert_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "# bert_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert_model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "def w2BERT(words, tokenizer=bert_tokenizer, model=bert_model):\n",
    "    # Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n",
    "    indexed_tokens = tokenizer.encode(\" \".join(words), add_special_tokens=False)\n",
    "    for i, idx in enumerate(indexed_tokens):\n",
    "        if idx == 100:\n",
    "            print(words[i]+\" is unknown\")\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor)\n",
    "        print(encoded_layers.shape)\n",
    "    return encoded_layers[0]\n",
    "\n",
    "def w2BERT2(words, tokenizer=bert_tokenizer, model=bert_model, index=1, pool=True):\n",
    "    encodings = torch.Tensor(len(words), 768)\n",
    "    for i, word in enumerate(words):\n",
    "        indexed_tokens = tokenizer.encode(word, add_special_tokens=True)\n",
    "        if 100 in indexed_tokens: print(word+\" is unknown\")\n",
    "#         print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "        with torch.no_grad():\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            encoded_layers, _ = model(tokens_tensor)\n",
    "        if pool:\n",
    "            encodings[i,:] = encoded_layers[0].mean(axis=0)\n",
    "        else:\n",
    "            encodings[i,:] = encoded_layers[0,index]\n",
    "#     encodings = encodings / encodings.norm(dim=1)[:, None]\n",
    "    return encodings\n",
    "\n",
    "print(bert_tokenizer.all_special_ids)\n",
    "print(bert_tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 768])\n",
      "Similarity with 'man'\n",
      "man\t1.0\n",
      "woman\t0.9406140446662903\n",
      "he\t0.9063779711723328\n",
      "she\t0.902837336063385\n",
      "farmer\t0.8989346027374268\n",
      "computer\t0.8856185078620911\n",
      "metal\t0.8848690986633301\n",
      "vehicle\t0.8794581294059753\n",
      "car\t0.8697401881217957\n",
      "bee\t0.861171543598175\n",
      "tractor\t0.8602543473243713\n",
      "programmer\t0.8572993874549866\n",
      "steak\t0.8522169589996338\n",
      "motor\t0.8498015403747559\n",
      "potato\t0.8467987179756165\n",
      "food\t0.8389607667922974\n",
      "fish\t0.8364245891571045\n",
      "water\t0.8343541026115417\n",
      "carrot\t0.8342726826667786\n"
     ]
    }
   ],
   "source": [
    "words = [\"man\", \"woman\", \"car\", \"tractor\", \"bee\", \"steak\", \"vehicle\", \"motor\", \"potato\", \"carrot\", \"food\", \"farmer\", \"fish\", \"water\", \"metal\", \"she\", \"he\", \"programmer\", \"computer\"]\n",
    "encodings = w2BERT2(words, index=0, pool=True)\n",
    "print(encodings.shape)\n",
    "\n",
    "similarity(words, encodings, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 4, 7, 6, 5, 0, 8]\n",
      "['<cls>', '</s>', '<s>', '<sep>', '<eod>', '<mask>', '<pad>', '<unk>', '<eop>']\n"
     ]
    }
   ],
   "source": [
    "xlnet_tokenizer = transformers.XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_model = transformers.XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "def w2xlnet(words, tokenizer=xlnet_tokenizer, model=xlnet_model):\n",
    "    indexed_tokens = tokenizer.encode(\"<sep>\".join(words), add_special_tokens=True)[:-1] # Exclude final <cls>\n",
    "    for i, idx in enumerate(indexed_tokens):\n",
    "        if idx == 0:\n",
    "            print(words[i]+\" is unknown\")\n",
    "    print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(tokens_tensor)\n",
    "    return encoded_layers[0]\n",
    "\n",
    "def w2xlnet2(words, tokenizer=xlnet_tokenizer, model=xlnet_model, index=0, pool=True):\n",
    "    encodings = torch.Tensor(len(words), 768)\n",
    "    for i, word in enumerate(words):\n",
    "        indexed_tokens = tokenizer.encode(word, add_special_tokens=True)\n",
    "        if 0 in indexed_tokens: print(word+\" is unknown\")\n",
    "#         print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "        with torch.no_grad():\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            encoded_layers, = model(tokens_tensor)\n",
    "        if pool:\n",
    "            encodings[i,:] = encoded_layers[0].mean(axis=0)\n",
    "        else:\n",
    "            encodings[i,:] = encoded_layers[0,index]\n",
    "#     encodings = encodings / encodings.norm(dim=1)[:, None]\n",
    "    return encodings\n",
    "\n",
    "print(xlnet_tokenizer.all_special_ids)\n",
    "print(xlnet_tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 768])\n",
      "Similarity with 'car'\n",
      "car\t1.0\n",
      "tractor\t0.9669960737228394\n",
      "computer\t0.964198112487793\n",
      "queen\t0.9582439661026001\n",
      "water\t0.9576461911201477\n",
      "fish\t0.957129955291748\n",
      "king\t0.9553025960922241\n",
      "potato\t0.9539452791213989\n",
      "vehicle\t0.9538944363594055\n",
      "metal\t0.9533132314682007\n",
      "motor\t0.9527180790901184\n",
      "farmer\t0.9509403109550476\n",
      "steak\t0.950559139251709\n",
      "carrot\t0.9503024816513062\n",
      "programmer\t0.9501314163208008\n",
      "woman\t0.9483902454376221\n",
      "he\t0.9483819603919983\n",
      "food\t0.9464730024337769\n",
      "man\t0.9384380578994751\n",
      "she\t0.9346848130226135\n",
      "bee\t0.920799732208252\n",
      "nerd\t0.901278555393219\n"
     ]
    }
   ],
   "source": [
    "words = [\"man\", \"woman\", \"car\", \"king\", \"queen\", \"tractor\", \"nerd\", \"bee\", \"steak\", \"vehicle\", \"motor\", \"potato\", \"carrot\", \"food\", \"farmer\", \"fish\", \"water\", \"metal\", \"she\", \"he\", \"programmer\", \"computer\"]\n",
    "encodings = w2xlnet2(words, index=1, pool=False)\n",
    "print(encodings.shape)\n",
    "\n",
    "similarity(words, encodings, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet/BERT words to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "<debiaswe.we.WordEmbedding object at 0x000002923F85E308>\n",
      "Words: 26423\n"
     ]
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "E = WordEmbedding('./embeddings/w2v_gnews_small.txt')\n",
    "print(E)\n",
    "words = E.words\n",
    "print(\"Words:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n"
     ]
    }
   ],
   "source": [
    "def save_w2XLNet(words, filename, index=0, pool=True, specials=True):\n",
    "    xlnet_tokenizer = transformers.XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    xlnet_model = transformers.XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "    unknowns = []\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(words):\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            f.write(word+\" \")\n",
    "            indexed_tokens = xlnet_tokenizer.encode(word, add_special_tokens=specials)\n",
    "            if 0 in indexed_tokens:\n",
    "                print(word+\" is unknown\")\n",
    "                unknowns.append(word)\n",
    "    #       print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "            with torch.no_grad():\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                encoded_layers, = xlnet_model(tokens_tensor) # Tensor\n",
    "                if pool:\n",
    "                    encoded_layers = encoded_layers[0].mean(axis=0)\n",
    "                else:\n",
    "                    encoded_layers = encoded_layers[0, index]\n",
    "                vector_list = encoded_layers.tolist()\n",
    "                vector_list = [str(x) for x in vector_list]\n",
    "                f.write(\" \".join(vector_list)+\"\\n\")\n",
    "\n",
    "save_w2XLNet(words, \"xlnet_specials_meanpool.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n"
     ]
    }
   ],
   "source": [
    "def save_w2Bert(words, filename, index=1, pool=True, specials=True):\n",
    "    bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    bert_model = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "    unknowns = []\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(words):\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            f.write(word+\" \")\n",
    "            indexed_tokens = bert_tokenizer.encode(word, add_special_tokens=specials)\n",
    "            if 0 in indexed_tokens:\n",
    "                print(word+\" is unknown\")\n",
    "                unknowns.append(word)\n",
    "    #       print(tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "            with torch.no_grad():\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                encoded_layers, _ = bert_model(tokens_tensor)\n",
    "                if pool:\n",
    "                    encoded_layers = encoded_layers[0].mean(axis=0)\n",
    "                else:\n",
    "                    encoded_layers = encoded_layers[0, index]\n",
    "                vector_list = encoded_layers.tolist()\n",
    "                vector_list = [str(x) for x in vector_list]\n",
    "                f.write(\" \".join(vector_list)+\"\\n\")\n",
    "\n",
    "save_w2Bert(words, \"bert_specials_meanpool.csv\", pool=True, specials=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
